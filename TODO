
- Support deleting/truncating indexes. Persistent workflow ensures operation completes.
- Online Database pruning, to reduce the file size.
- Online Index compaction, reducing the b-tree overhead.
- Use concurrent queue algorithms for binding/unbinding cursor frames,
  eliminating exclusive latch requirement.
- Support key prefix compression.
- Support snapshot into a temp Database instance.
- Use 6-byte pointers in internal nodes.
- Add Index.insertFence and removeFence methods for boosting findNearby
  performance at the edges. Implementation inserts a ghost.


Reduce overhead of checkpoint by sweeping through dirty nodes twice. This
approach smooths out the "bump" when issuing a checkpoint. Also, it allows
dirty nodes to move to MRU position when accessed. A new state is introduced,
"mutable". At checkpoint, sweep through all dirty nodes, write them out, and
mark them as mutable. When a mutable node is altered, its state becomes dirty
again, but it doesn't acquire a new identifier. After first checkpoint sweep,
switch active state (0/1). All dirty nodes with old state are written first
when modified, just like the current design. Mutable nodes with old state don't
need to be written, but a new identifier must still be acquired. Checkpoint now
sweeps through node list again, forcing all dirty nodes to be written and
marked clean. Those which are mutable are also marked clean, but without the
additional write.

Shared commit lock is held for a long time, especially when a fragmented value
is being created. When the checkpointer requests an exclusive lock, all other
requests for the shared lock are blocked. Introduce a new type of lock, which
is hooked into the FileIO or PageArray class. When an I/O operation is
performed, assume it will take a long time and allow any shared lock waiters to
barge ahead of the exclusive lock request.

Support page-level compression. The easiest way to do this is to define a new
CompressedPageArray class. It maps logical page ids to compressed pages, stored
in another database. The logical and compressed databases must have different
page sizes to be effective. The best ratio needs to be determined, but I think
512 byte pages would work best for the compressed database. If the logical page
size is less than 4096, then ratio should probably be swapped -- compressed
page should 8 times larger than logical page size.

---
